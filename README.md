# ğŸ“ Workshop: Getting Started with Ollama & Running LLaMA Models

Welcome to our hands-on workshop! This guide walks you through installing **Ollama**, running **LLaMA models**, customizing behavior, and backing up results â€” all beginner-friendly!

---

## ğŸ¤ Self Introduction

### ğŸ‘¨â€ğŸ’» SK. Muneer â€“ *Tech Guide*

Hello everyone! Iâ€™m SK. Muneer, a passionate technology educator and AI enthusiast.
I specialize in making complex technologies approachable and hands-on.
Today, Iâ€™ll guide you through using **Ollama** and **LLaMA** models to build powerful AI applications.

### ğŸ‘©â€ğŸ’» Aishwarya â€“ *Clarity Coach*

Hi there! Iâ€™m Aishwarya â€” a software engineer and advocate for open-source learning.
I focus on simplifying technical concepts so anyone can explore and innovate with AI.
Weâ€™re here to ensure you get real results with practical demos, not just theory.

---

## ğŸ§½ Table of Contents

1. [What is Ollama?](#what-is-ollama)
2. [Why Use Ollama?](#why-use-ollama)
3. [What Youâ€™ll Need](#what-youll-need)
4. [Installing Ollama](#installing-ollama)
5. [Running LLaMA Models](#running-llama-models)
6. [Customizing Model Behavior](#customizing-model-behavior)
7. [Understanding Responses](#understanding-responses)
8. [Backing Up Outputs to Google Drive](#backing-up-outputs-to-google-drive)
9. [Troubleshooting](#troubleshooting)
10. [Contributing to Ollama](#contributing-to-ollama)
11. [License](#license)
12. [Contact](#contact)
13. [Appendix: Full Install Script](#appendix-full-install-script)

---

## ğŸ§  What is Ollama?

**Ollama** is an open-source tool that lets you run LLMs (like LLaMA3) locally â€” no internet required post-setup.

* ğŸ” Private by default
* âš¡ Fast & efficient
* ğŸ§  Supports popular models
* ğŸ’½ Works on macOS, Linux & Windows (via WSL2)

---

## ğŸ’¡ Why Use Ollama?

| Feature          | Benefit                                  |
| ---------------- | ---------------------------------------- |
| âœ… Runs Locally   | Full offline capabilities after download |
| ğŸ” Private       | No data leaves your device               |
| ğŸš€ Simple Setup  | Works across platforms                   |
| ğŸ§  Model Variety | LLaMA, Mistral, and more supported       |
| â†º Dev Friendly   | Python integration and automation ready  |

---

## ğŸ§° What Youâ€™ll Need

* ğŸ’» macOS / Linux / Windows (WSL2)
* ğŸ“€ Minimum 8GB RAM (16GB+ recommended)
* ğŸŒ Internet for downloading models
* ğŸ§‘â€ğŸ’¼ Basic Terminal skills

---

## âš™ï¸ Installing Ollama

### ğŸ—… macOS

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### ğŸ§ Linux (Debian/Ubuntu-based)

```bash
sudo apt update
sudo apt install -y curl
curl -fsSL https://ollama.com/install.sh | sh
```

### ğŸª  Windows (via WSL2)

1. Install **Ubuntu from the Microsoft Store**
2. Launch Ubuntu and run:

```bash
sudo apt update
sudo apt install -y curl
curl -fsSL https://ollama.com/install.sh | sh
```

### âœ… Verify Installation

```bash
ollama --version
```

---

## ğŸ§ª Running LLaMA Models

### Step 1: Pull the model

```bash
ollama pull llama3
```

### Step 2: Run the model

```bash
ollama run llama3
```

Try:

```text
Tell me a joke.
Explain black holes in simple terms.
```

---

## ğŸ›ï¸ Customizing Model Behavior

Use the Python API to fine-tune how the model responds:

```python
import ollama

response = ollama.generate(
    model="llama3",
    prompt="Write a poem about cats.",
    temperature=0.8
)

print(response["response"])
```

| Parameter           | Description                |
| ------------------- | -------------------------- |
| `temperature`       | Higher = more creative     |
| `max_tokens`        | Limits response length     |
| `top_p`             | Controls diversity         |
| `frequency_penalty` | Reduces repetitive outputs |

---

## ğŸ“– Understanding Responses

Ollama can:

* ğŸ§  Answer questions
* ğŸ’» Generate and explain code
* ğŸ“š Summarize documents
* âœï¸ Write creative stories or poems
* ğŸ“Š Explain data and concepts clearly

> âš ï¸ Always verify important info â€” it may occasionally hallucinate or guess.

---

## â˜ï¸ Backing Up Outputs to Google Drive

### Step 1: Install rclone

```bash
sudo apt install rclone
rclone config
```

Configure it with your **Google Drive** account.

### Step 2: Create a local folder

```bash
mkdir ~/ollama_output
```

### Step 3: Save responses and upload

```bash
ollama generate llama3 "Your question" > ~/ollama_output/response.txt
rclone copy ~/ollama_output remote_drive:/ollama_backup
```

---

## ğŸ› ï¸ Troubleshooting

| Problem                | Solution                                      |
| ---------------------- | --------------------------------------------- |
| âŒ Can't download model | Check internet connection                     |
| ğŸ§  Out of memory error | Try a smaller model: `llama3:8b`              |
| âš ï¸ Command not found   | Reinstall Ollama or add it to your PATH       |
| ğŸš« GPU not used        | Ensure your CUDA/NVIDIA drivers are installed |

---

## ğŸ¤ Contributing to Ollama

Ollama is fully **open-source**. You can:

* ğŸ Report issues
* ğŸ’¡ Suggest features
* ğŸ“™ Improve docs
* ğŸ’» Submit PRs

ğŸ”— [GitHub â€“ ollama/ollama](https://github.com/ollama/ollama)

---

## ğŸ“„ License

* Ollama uses the **MIT License** â€” use, modify, and share freely.
* Each model (e.g., LLaMA3) may have its own license â€” review before redistribution.

---

## ğŸ“¬ Contact

| Name      | Email                                                 | GitHub Handle                                  |
| --------- | ----------------------------------------------------- | ---------------------------------------------- |
| Muneer    | [muneer@example.com](mailto:muneer@example.com)       | [@muneerai](https://github.com/muneerai)       |
| Aishwarya | [aishwarya@example.com](mailto:aishwarya@example.com) | [@aishwaryaml](https://github.com/aishwaryaml) |

---

## ğŸ“¦ Appendix: Full Install Script

Below is the actual install script used by Ollama (as of June 2025):

```bash
#!/bin/sh
set -e

ARCH=$(uname -m)
OS=$(uname -s | tr '[:upper:]' '[:lower:]')
VERSION="latest"

if [ "$ARCH" = "x86_64" ]; then
  ARCH="amd64"
elif [ "$ARCH" = "aarch64" ]; then
  ARCH="arm64"
fi

echo "Detected architecture: $ARCH"
echo "Detected OS: $OS"

curl -L "https://ollama.com/download/$OS/$ARCH" -o ollama.tar.gz
mkdir -p ~/.ollama
tar -xzf ollama.tar.gz -C ~/.ollama
sudo mv ~/.ollama/ollama /usr/local/bin/ollama
rm -rf ~/.ollama ollama.tar.gz

echo "âœ… Ollama installed successfully!"
```

---

## ğŸ‰ Final Words

Thank you for attending the workshop!

> ğŸ§  AI is no longer just for experts â€” it's for *everyone* whoâ€™s curious, creative, and ready to build.

Go ahead, experiment with LLaMA, build something cool â€” and have fun! ğŸš€
